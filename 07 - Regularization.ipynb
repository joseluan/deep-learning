{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regularization-intro",
   "metadata": {},
   "source": [
    "# Regularização\n",
    "\n",
    "Este notebook apresenta as principais técnicas de regularização utilizadas em deep learning para combater o overfitting e melhorar a generalização dos modelos. Abordaremos a implementação prática usando PyTorch.\n",
    "\n",
    "## Conteúdos Abordados\n",
    "\n",
    "1. **Introdução ao Overfitting**: Compreendendo o problema\n",
    "2. **Dropout**: Regularização através de desativação aleatória\n",
    "3. **Penalidades de Peso (L1 e L2)**: Weight decay e sparsidade\n",
    "4. **Early Stopping**: Parada antecipada baseada na validação\n",
    "5. **Data Augmentation**: Aumentando a diversidade dos dados\n",
    "6. **Comparação das Técnicas**: Análise experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup e Preparação dos Dados\n",
    "\n",
    "Começamos importando as bibliotecas necessárias e preparando um dataset reduzido do MNIST para experimentos rápidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "# Configurações\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Seed para reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "data-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino: 4000 amostras\n",
      "Dataset de validação: 1000 amostras\n"
     ]
    }
   ],
   "source": [
    "# Preparação do dataset MNIST reduzido\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Dataset completo\n",
    "full_train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "full_test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Criando subsets reduzidos para experimentos rápidos\n",
    "train_indices = torch.randperm(len(full_train_dataset))[:4000]\n",
    "val_indices = torch.randperm(len(full_test_dataset))[:1000]\n",
    "\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "val_dataset = Subset(full_test_dataset, val_indices)\n",
    "\n",
    "print(f\"Dataset de treino: {len(train_dataset)} amostras\")\n",
    "print(f\"Dataset de validação: {len(val_dataset)} amostras\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfitting-intro",
   "metadata": {},
   "source": [
    "## 1. Introdução ao Overfitting\n",
    "\n",
    "O **overfitting** ocorre quando um modelo aprende muito bem os dados de treinamento, incluindo o ruído e padrões específicos desses dados, mas falha em generalizar para dados novos. Matematicamente, isso acontece quando a complexidade do modelo é muito alta em relação à quantidade de dados disponíveis.\n",
    "\n",
    "O **bias-variance tradeoff** é fundamental para entender este fenômeno:\n",
    "\n",
    "$$\\text{Erro Total} = \\text{Bias}^2 + \\text{Variance} + \\text{Ruído Irredutível}$$\n",
    "\n",
    "- **Bias**: erro devido a suposições simplificadoras no algoritmo\n",
    "- **Variance**: erro devido à sensibilidade a pequenas flutuações nos dados de treino\n",
    "- **Ruído Irredutível**: erro inerente ao problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo base simples para demonstrar overfitting\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"Função geral de treinamento\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calcular métricas da época\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Época [{epoch+1}/{num_epochs}] - '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% - '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Função de avaliação do modelo\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-experiment",
   "metadata": {},
   "source": [
    "### Experimento Baseline: Modelo sem Regularização\n",
    "\n",
    "Primeiro, vamos treinar um modelo sem nenhuma técnica de regularização para estabelecer uma baseline e observar o comportamento do overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo baseline sem regularização\n",
    "baseline_model = BaseModel(dropout_rate=0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Treinando modelo baseline (sem regularização)...\")\n",
    "baseline_train_losses, baseline_val_losses, baseline_train_acc, baseline_val_acc = train_model(\n",
    "    baseline_model, train_loader, val_loader, criterion, optimizer, num_epochs=30\n",
    ")\n",
    "\n",
    "# Plotar curvas de aprendizado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perda\n",
    "ax1.plot(baseline_train_losses, label='Treino', color='blue')\n",
    "ax1.plot(baseline_val_losses, label='Validação', color='red')\n",
    "ax1.set_title('Curvas de Perda - Modelo Baseline')\n",
    "ax1.set_xlabel('Época')\n",
    "ax1.set_ylabel('Perda')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Acurácia\n",
    "ax2.plot(baseline_train_acc, label='Treino', color='blue')\n",
    "ax2.plot(baseline_val_acc, label='Validação', color='red')\n",
    "ax2.set_title('Curvas de Acurácia - Modelo Baseline')\n",
    "ax2.set_xlabel('Época')\n",
    "ax2.set_ylabel('Acurácia (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "baseline_val_acc_final, baseline_val_loss_final = evaluate_model(baseline_model, val_loader)\n",
    "print(f\"\\nDesempenho final do modelo baseline:\")\n",
    "print(f\"Acurácia de validação: {baseline_val_acc_final:.2f}%\")\n",
    "print(f\"Perda de validação: {baseline_val_loss_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dropout-theory",
   "metadata": {},
   "source": [
    "## 2. Dropout\n",
    "\n",
    "O **Dropout** é uma técnica de regularização que, durante o treinamento, desativa aleatoriamente uma fração dos neurônios em cada forward pass. Isso força o modelo a não depender excessivamente de neurônios específicos, melhorando a generalização.\n",
    "\n",
    "Durante o treinamento, para uma camada com entrada $\\mathbf{x}$:\n",
    "\n",
    "$$\\tilde{\\mathbf{x}} = \\mathbf{r} \\odot \\mathbf{x}$$\n",
    "\n",
    "onde:\n",
    "- $\\mathbf{r} \\sim \\text{Bernoulli}(p)$ é um vetor de máscara aleatória\n",
    "- $p$ é a probabilidade de manter um neurônio ativo\n",
    "- $\\odot$ denota produto elemento a elemento (Hadamard)\n",
    "\n",
    "Durante a inferência, todos os neurônios são mantidos ativos, mas suas saídas são escalonadas por $p$ para compensar:\n",
    "\n",
    "$$\\mathbf{x}_{\\text{test}} = p \\cdot \\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropout-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo com dropout\n",
    "dropout_model = BaseModel(dropout_rate=0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(dropout_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Treinando modelo com Dropout (rate=0.3)...\")\n",
    "dropout_train_losses, dropout_val_losses, dropout_train_acc, dropout_val_acc = train_model(\n",
    "    dropout_model, train_loader, val_loader, criterion, optimizer, num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropout-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização do efeito do dropout\n",
    "def visualize_dropout_effect(model, input_tensor, num_samples=10):\n",
    "    \"\"\"Visualiza como o dropout afeta as ativações\"\"\"\n",
    "    model.train()  # Ativar dropout\n",
    "    \n",
    "    activations = []\n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            x = model.flatten(input_tensor)\n",
    "            x = F.relu(model.fc1(x))\n",
    "            x = model.dropout1(x)  # Aplicar dropout\n",
    "            activations.append(x.cpu().numpy())\n",
    "    \n",
    "    return np.array(activations)\n",
    "\n",
    "# Pegar uma amostra do dataset\n",
    "sample_data, _ = next(iter(val_loader))\n",
    "sample = sample_data[0:1].to(device)  # Primeira amostra\n",
    "\n",
    "# Comparar ativações com e sem dropout\n",
    "dropout_activations = visualize_dropout_effect(dropout_model, sample)\n",
    "\n",
    "# Plotar distribuição das ativações\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Ativações com dropout (múltiplas amostras)\n",
    "axes[0, 0].hist(dropout_activations[0].flatten(), bins=50, alpha=0.7, color='blue', label='Amostra 1')\n",
    "axes[0, 0].hist(dropout_activations[1].flatten(), bins=50, alpha=0.7, color='red', label='Amostra 2')\n",
    "axes[0, 0].set_title('Ativações com Dropout (diferentes execuções)')\n",
    "axes[0, 0].set_xlabel('Valor da Ativação')\n",
    "axes[0, 0].set_ylabel('Frequência')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Comparação da variabilidade\n",
    "std_per_neuron = np.std(dropout_activations, axis=0).flatten()\n",
    "axes[0, 1].hist(std_per_neuron, bins=30, color='green', alpha=0.7)\n",
    "axes[0, 1].set_title('Variabilidade por Neurônio (Desvio Padrão)')\n",
    "axes[0, 1].set_xlabel('Desvio Padrão das Ativações')\n",
    "axes[0, 1].set_ylabel('Número de Neurônios')\n",
    "\n",
    "# Comparação das curvas de aprendizado\n",
    "axes[1, 0].plot(baseline_train_losses, label='Baseline - Treino', color='blue', linestyle='--')\n",
    "axes[1, 0].plot(baseline_val_losses, label='Baseline - Val', color='blue', linestyle='-')\n",
    "axes[1, 0].plot(dropout_train_losses, label='Dropout - Treino', color='red', linestyle='--')\n",
    "axes[1, 0].plot(dropout_val_losses, label='Dropout - Val', color='red', linestyle='-')\n",
    "axes[1, 0].set_title('Comparação: Baseline vs Dropout (Perda)')\n",
    "axes[1, 0].set_xlabel('Época')\n",
    "axes[1, 0].set_ylabel('Perda')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(baseline_train_acc, label='Baseline - Treino', color='blue', linestyle='--')\n",
    "axes[1, 1].plot(baseline_val_acc, label='Baseline - Val', color='blue', linestyle='-')\n",
    "axes[1, 1].plot(dropout_train_acc, label='Dropout - Treino', color='red', linestyle='--')\n",
    "axes[1, 1].plot(dropout_val_acc, label='Dropout - Val', color='red', linestyle='-')\n",
    "axes[1, 1].set_title('Comparação: Baseline vs Dropout (Acurácia)')\n",
    "axes[1, 1].set_xlabel('Época')\n",
    "axes[1, 1].set_ylabel('Acurácia (%)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "dropout_val_acc_final, dropout_val_loss_final = evaluate_model(dropout_model, val_loader)\n",
    "print(f\"\\nDesempenho do modelo com Dropout:\")\n",
    "print(f\"Acurácia de validação: {dropout_val_acc_final:.2f}%\")\n",
    "print(f\"Perda de validação: {dropout_val_loss_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weight-penalties-theory",
   "metadata": {},
   "source": [
    "## 3. Penalidades de Peso (L1 e L2)\n",
    "\n",
    "As **penalidades de peso** adicionam um termo de regularização à função de custo original, penalizando pesos com magnitudes grandes. Isso encoraja o modelo a aprender representações mais simples.\n",
    "\n",
    "### Regularização L2 (Weight Decay)\n",
    "\n",
    "A regularização L2 adiciona o quadrado da magnitude dos pesos à função de custo:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{original}} + \\lambda \\sum_{i} w_i^2$$\n",
    "\n",
    "onde $\\lambda$ é o coeficiente de regularização. O gradiente do termo de regularização é:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_i}(\\lambda w_i^2) = 2\\lambda w_i$$\n",
    "\n",
    "Isso leva à regra de atualização:\n",
    "\n",
    "$$w_i \\leftarrow w_i - \\eta(\\nabla \\mathcal{L}_{\\text{original}} + 2\\lambda w_i) = (1 - 2\\eta\\lambda)w_i - \\eta\\nabla \\mathcal{L}_{\\text{original}}$$\n",
    "\n",
    "### Regularização L1 (Lasso)\n",
    "\n",
    "A regularização L1 adiciona a magnitude absoluta dos pesos:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{original}} + \\lambda \\sum_{i} |w_i|$$\n",
    "\n",
    "O gradiente (subgradiente) do termo L1 é:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_i}(\\lambda |w_i|) = \\lambda \\text{sign}(w_i)$$\n",
    "\n",
    "A regularização L1 tende a produzir **sparsidade**, zerando muitos pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weight-decay-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de treinamento com penalidades L1 e L2\n",
    "def train_model_with_penalties(model, train_loader, val_loader, criterion, optimizer, \n",
    "                               l1_lambda=0, l2_lambda=0, num_epochs=10):\n",
    "    \"\"\"Treinamento com penalidades L1 e L2 explícitas\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Adicionar penalidades L1 e L2\n",
    "            l1_penalty = sum(torch.sum(torch.abs(p)) for p in model.parameters()) if l1_lambda > 0 else 0\n",
    "            l2_penalty = sum(torch.sum(p ** 2) for p in model.parameters()) if l2_lambda > 0 else 0\n",
    "\n",
    "            total_loss = loss + l1_lambda * l1_penalty + l2_lambda * l2_penalty\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()  # Apenas a perda original para comparação\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Validação (sem penalidades)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calcular métricas da época\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Época [{epoch+1}/{num_epochs}] - '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% - '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento com regularização L2\n",
    "l2_model = BaseModel(dropout_rate=0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(l2_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Treinando modelo com regularização L2 (lambda=0.01)...\")\n",
    "l2_train_losses, l2_val_losses, l2_train_acc, l2_val_acc = train_model_with_penalties(\n",
    "    l2_model, train_loader, val_loader, criterion, optimizer, \n",
    "    l1_lambda=0, l2_lambda=0.01, num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento com regularização L1\n",
    "l1_model = BaseModel(dropout_rate=0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(l1_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Treinando modelo com regularização L1 (lambda=0.001)...\")\n",
    "l1_train_losses, l1_val_losses, l1_train_acc, l1_val_acc = train_model_with_penalties(\n",
    "    l1_model, train_loader, val_loader, criterion, optimizer, \n",
    "    l1_lambda=0.001, l2_lambda=0, num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weight-distribution-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da distribuição dos pesos\n",
    "def plot_weight_distributions(models, labels, colors):\n",
    "    \"\"\"Plota a distribuição dos pesos de diferentes modelos\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    for idx, (model, label, color) in enumerate(zip(models, labels, colors)):\n",
    "        # Coletar todos os pesos do modelo\n",
    "        all_weights = []\n",
    "        for param in model.parameters():\n",
    "            all_weights.extend(param.detach().cpu().numpy().flatten())\n",
    "        \n",
    "        all_weights = np.array(all_weights)\n",
    "        \n",
    "        # Histograma\n",
    "        axes[0, 0].hist(all_weights, bins=50, alpha=0.7, label=label, color=color, density=True)\n",
    "        \n",
    "        # Estatísticas\n",
    "        mean_weight = np.mean(np.abs(all_weights))\n",
    "        std_weight = np.std(all_weights)\n",
    "        sparsity = np.mean(np.abs(all_weights) < 1e-3) * 100  # Porcentagem de pesos \"zero\"\n",
    "        \n",
    "        print(f\"{label}:\")\n",
    "        print(f\"  Magnitude média: {mean_weight:.6f}\")\n",
    "        print(f\"  Desvio padrão: {std_weight:.6f}\")\n",
    "        print(f\"  Sparsidade: {sparsity:.2f}%\")\n",
    "        print()\n",
    "    \n",
    "    axes[0, 0].set_title('Distribuição dos Pesos')\n",
    "    axes[0, 0].set_xlabel('Valor do Peso')\n",
    "    axes[0, 0].set_ylabel('Densidade')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Zoom na região central\n",
    "    for idx, (model, label, color) in enumerate(zip(models, labels, colors)):\n",
    "        all_weights = []\n",
    "        for param in model.parameters():\n",
    "            all_weights.extend(param.detach().cpu().numpy().flatten())\n",
    "        \n",
    "        all_weights = np.array(all_weights)\n",
    "        axes[0, 1].hist(all_weights, bins=50, alpha=0.7, label=label, color=color, \n",
    "                       range=(-0.5, 0.5), density=True)\n",
    "    \n",
    "    axes[0, 1].set_title('Distribuição dos Pesos (Zoom Central)')\n",
    "    axes[0, 1].set_xlabel('Valor do Peso')\n",
    "    axes[0, 1].set_ylabel('Densidade')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Comparação das curvas de validação\n",
    "    curves = [baseline_val_losses, l2_val_losses, l1_val_losses]\n",
    "    for curve, label, color in zip(curves, labels, colors):\n",
    "        axes[1, 0].plot(curve, label=label, color=color)\n",
    "    \n",
    "    axes[1, 0].set_title('Curvas de Perda de Validação')\n",
    "    axes[1, 0].set_xlabel('Época')\n",
    "    axes[1, 0].set_ylabel('Perda')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Comparação das curvas de acurácia\n",
    "    curves = [baseline_val_acc, l2_val_acc, l1_val_acc]\n",
    "    for curve, label, color in zip(curves, labels, colors):\n",
    "        axes[1, 1].plot(curve, label=label, color=color)\n",
    "    \n",
    "    axes[1, 1].set_title('Curvas de Acurácia de Validação')\n",
    "    axes[1, 1].set_xlabel('Época')\n",
    "    axes[1, 1].set_ylabel('Acurácia (%)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotar comparações\n",
    "models = [baseline_model, l2_model, l1_model]\n",
    "labels = ['Baseline', 'L2 Regularization', 'L1 Regularization']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "plot_weight_distributions(models, labels, colors)\n",
    "\n",
    "# Avaliar desempenho final\n",
    "l2_val_acc_final, l2_val_loss_final = evaluate_model(l2_model, val_loader)\n",
    "l1_val_acc_final, l1_val_loss_final = evaluate_model(l1_model, val_loader)\n",
    "\n",
    "print(f\"Desempenho final - Regularização L2:\")\n",
    "print(f\"Acurácia de validação: {l2_val_acc_final:.2f}%\")\n",
    "print(f\"Perda de validação: {l2_val_loss_final:.4f}\")\n",
    "print()\n",
    "print(f\"Desempenho final - Regularização L1:\")\n",
    "print(f\"Acurácia de validação: {l1_val_acc_final:.2f}%\")\n",
    "print(f\"Perda de validação: {l1_val_loss_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-stopping-theory",
   "metadata": {},
   "source": [
    "## 4. Early Stopping\n",
    "\n",
    "O **Early Stopping** é uma técnica de regularização que interrompe o treinamento quando a performance no conjunto de validação para de melhorar, evitando que o modelo continue aprendendo padrões específicos dos dados de treino.\n",
    "\n",
    "### Critérios de Parada\n",
    "\n",
    "O early stopping monitora uma métrica de validação $V(t)$ (como perda ou acurácia) ao longo das épocas $t$. O treinamento é interrompido quando:\n",
    "\n",
    "$$V(t) > V(t-p) + \\delta$$\n",
    "\n",
    "onde:\n",
    "- $p$ é a **paciência** (número de épocas sem melhoria)\n",
    "- $\\delta$ é a **tolerância mínima** de melhoria\n",
    "\n",
    "Esta técnica implementa implicitamente a regularização ao limitar a capacidade do modelo de se ajustar excessivamente aos dados de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-stopping-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, \n",
    "                             patience=5, min_delta=0.001, max_epochs=50):\n",
    "    \"\"\"Treinamento com early stopping\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calcular métricas da época\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        # Verificar early stopping\n",
    "        if epoch_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Época [{epoch+1}/{max_epochs}] - '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% - '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}% - '\n",
    "                  f'Paciência: {epochs_without_improvement}/{patience}')\n",
    "        \n",
    "        # Parar se a paciência se esgotou\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping na época {epoch + 1}\")\n",
    "            print(f\"Melhor perda de validação: {best_val_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Restaurar o melhor modelo\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Modelo restaurado para o melhor estado\")\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-stopping-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento com early stopping\n",
    "early_stop_model = BaseModel(dropout_rate=0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(early_stop_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Treinando modelo com Early Stopping (paciência=7)...\")\n",
    "es_train_losses, es_val_losses, es_train_acc, es_val_acc, final_epoch = train_with_early_stopping(\n",
    "    early_stop_model, train_loader, val_loader, criterion, optimizer, \n",
    "    patience=7, min_delta=0.001, max_epochs=50\n",
    ")\n",
    "\n",
    "# Plotar resultados\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Comparação das perdas\n",
    "epochs_baseline = range(len(baseline_train_losses))\n",
    "epochs_es = range(len(es_train_losses))\n",
    "\n",
    "ax1.plot(epochs_baseline, baseline_train_losses, label='Baseline - Treino', color='blue', linestyle='--')\n",
    "ax1.plot(epochs_baseline, baseline_val_losses, label='Baseline - Val', color='blue', linestyle='-')\n",
    "ax1.plot(epochs_es, es_train_losses, label='Early Stop - Treino', color='red', linestyle='--')\n",
    "ax1.plot(epochs_es, es_val_losses, label='Early Stop - Val', color='red', linestyle='-')\n",
    "ax1.axvline(x=final_epoch-1, color='red', linestyle=':', alpha=0.7, label='Parada')\n",
    "ax1.set_title('Comparação: Baseline vs Early Stopping (Perda)')\n",
    "ax1.set_xlabel('Época')\n",
    "ax1.set_ylabel('Perda')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Comparação das acurácias\n",
    "ax2.plot(epochs_baseline, baseline_train_acc, label='Baseline - Treino', color='blue', linestyle='--')\n",
    "ax2.plot(epochs_baseline, baseline_val_acc, label='Baseline - Val', color='blue', linestyle='-')\n",
    "ax2.plot(epochs_es, es_train_acc, label='Early Stop - Treino', color='red', linestyle='--')\n",
    "ax2.plot(epochs_es, es_val_acc, label='Early Stop - Val', color='red', linestyle='-')\n",
    "ax2.axvline(x=final_epoch-1, color='red', linestyle=':', alpha=0.7, label='Parada')\n",
    "ax2.set_title('Comparação: Baseline vs Early Stopping (Acurácia)')\n",
    "ax2.set_xlabel('Época')\n",
    "ax2.set_ylabel('Acurácia (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "es_val_acc_final, es_val_loss_final = evaluate_model(early_stop_model, val_loader)\n",
    "print(f\"\\nDesempenho do modelo com Early Stopping:\")\n",
    "print(f\"Acurácia de validação: {es_val_acc_final:.2f}%\")\n",
    "print(f\"Perda de validação: {es_val_loss_final:.4f}\")\n",
    "print(f\"Épocas treinadas: {final_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-augmentation-theory",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation\n",
    "\n",
    "O **Data Augmentation** é uma técnica que artificialmente aumenta o tamanho do dataset através de transformações que preservam a classe dos dados. Para imagens, isso inclui rotações, translações, mudanças de brilho, etc.\n",
    "\n",
    "### Fundamentação Teórica\n",
    "\n",
    "Data augmentation implementa uma forma de regularização ao expor o modelo a variações realísticas dos dados de treino. Matematicamente, se $\\mathcal{T}$ é um conjunto de transformações válidas, então para cada amostra $(x, y)$, geramos:\n",
    "\n",
    "$$\\{(T(x), y) : T \\in \\mathcal{T}\\}$$\n",
    "\n",
    "Isso efetivamente aumenta o tamanho do dataset de $N$ para $N \\times |\\mathcal{T}|$ amostras, melhorando a capacidade de generalização.\n",
    "\n",
    "### Invariâncias\n",
    "\n",
    "O data augmentation ensina ao modelo **invariâncias importantes**:\n",
    "- **Invariância translacional**: o objeto continua o mesmo independente da posição\n",
    "- **Invariância rotacional**: pequenas rotações não alteram a classe\n",
    "- **Invariância de iluminação**: mudanças de brilho não afetam a identidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-augmentation-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo transformações de data augmentation\n",
    "train_transform_augmented = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # Rotação aleatória de até 10 graus\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.2, 0.2)),  # Translação aleatória\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Dataset com data augmentation\n",
    "train_dataset_augmented = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, transform=train_transform_augmented, download=False\n",
    ")\n",
    "train_dataset_augmented = Subset(train_dataset_augmented, train_indices)\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Visualizar algumas amostras aumentadas\n",
    "def show_augmented_samples(dataset, original_dataset, num_samples=8):\n",
    "    \"\"\"Mostra comparação entre amostras originais e aumentadas\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 4))\n",
    "    \n",
    "    # Escolher índices aleatórios\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Amostra original\n",
    "        orig_img, label = original_dataset[idx]\n",
    "        orig_img = orig_img * 0.3081 + 0.1307  # Desnormalizar\n",
    "        \n",
    "        # Amostra aumentada\n",
    "        aug_img, _ = dataset[idx]\n",
    "        aug_img = aug_img * 0.3081 + 0.1307  # Desnormalizar\n",
    "        \n",
    "        axes[0, i].imshow(orig_img.squeeze(), cmap='gray')\n",
    "        axes[0, i].set_title(f'Original ({label})')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(aug_img.squeeze(), cmap='gray')\n",
    "        axes[1, i].set_title('Aumentada')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_augmented_samples(train_dataset_augmented, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-augmentation-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento com data augmentation\n",
    "aug_model = BaseModel(dropout_rate=0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(aug_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Treinando modelo com Data Augmentation...\")\n",
    "aug_train_losses, aug_val_losses, aug_train_acc, aug_val_acc = train_model(\n",
    "    aug_model, train_loader_augmented, val_loader, criterion, optimizer, num_epochs=30\n",
    ")\n",
    "\n",
    "# Plotar comparação\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Comparação das perdas\n",
    "ax1.plot(baseline_train_losses, label='Baseline - Treino', color='blue', linestyle='--')\n",
    "ax1.plot(baseline_val_losses, label='Baseline - Val', color='blue', linestyle='-')\n",
    "ax1.plot(aug_train_losses, label='Data Aug - Treino', color='green', linestyle='--')\n",
    "ax1.plot(aug_val_losses, label='Data Aug - Val', color='green', linestyle='-')\n",
    "ax1.set_title('Comparação: Baseline vs Data Augmentation (Perda)')\n",
    "ax1.set_xlabel('Época')\n",
    "ax1.set_ylabel('Perda')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Comparação das acurácias\n",
    "ax2.plot(baseline_train_acc, label='Baseline - Treino', color='blue', linestyle='--')\n",
    "ax2.plot(baseline_val_acc, label='Baseline - Val', color='blue', linestyle='-')\n",
    "ax2.plot(aug_train_acc, label='Data Aug - Treino', color='green', linestyle='--')\n",
    "ax2.plot(aug_val_acc, label='Data Aug - Val', color='green', linestyle='-')\n",
    "ax2.set_title('Comparação: Baseline vs Data Augmentation (Acurácia)')\n",
    "ax2.set_xlabel('Época')\n",
    "ax2.set_ylabel('Acurácia (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "aug_val_acc_final, aug_val_loss_final = evaluate_model(aug_model, val_loader)\n",
    "print(f\"\\nDesempenho do modelo com Data Augmentation:\")\n",
    "print(f\"Acurácia de validação: {aug_val_acc_final:.2f}%\")\n",
    "print(f\"Perda de validação: {aug_val_loss_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "1. **Dropout**: Eficaz e fácil de implementar. Funciona bem em redes totalmente conectadas e pode ser ajustado via taxa de dropout.\n",
    "\n",
    "2. **Regularização L2 (Weight Decay)**: Suaviza os pesos, prevenindo valores extremos. Amplamente usado e computacionalmente eficiente.\n",
    "\n",
    "3. **Regularização L1**: Promove sparsidade, útil para seleção automática de features. Pode ser mais difícil de otimizar.\n",
    "\n",
    "4. **Early Stopping**: Simples e efetivo, mas requer conjunto de validação separado e monitoramento cuidadoso.\n",
    "\n",
    "5. **Data Augmentation**: Especialmente poderoso para dados visuais, mas requer conhecimento do domínio para escolher transformações apropriadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "### Exercício 1: Experimentação com Hyperparâmetros\n",
    "Teste diferentes valores de dropout rate e compare os resultados. Qual valor funciona melhor para este dataset?\n",
    "\n",
    "### Exercício 2: Combinação de Técnicas\n",
    "Crie um modelo que combine dropout com regularização L2. Compare com os modelos individuais.\n",
    "\n",
    "### Exercício 3: Data Augmentation Customizado\n",
    "Experimente diferentes transformações de data augmentation (veja a documentação do TorchVision).\n",
    "\n",
    "### Exercício 4: Análise de Overfitting\n",
    "Use um dataset ainda menor (1000 amostras de treino) e compare como cada técnica de regularização se comporta em um cenário de overfitting mais severo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
